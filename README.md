# Отчет по лабораторной работе 5

Отчет по лабораторной работе #5 выполнил(а):
- Удовенко Софья Вадимовна
- РИ-220932
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

## Цель работы
Познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.

## Задание 1
Ход работы:
- Найти внутри C# скрипта “коэффициент корреляции ” и сделать выводы о том, как он влияет на обучение модели.

В данном коде коэффициент корреляции присутствует между состоянием агента и его действиями.
Состояние агента включает в себя:
- Позицию цели (Target.localPosition)
- Собственную позицию агента (this.transform.localPosition)
- Скорость агента (rBody.velocity)

Действия агента - это значения, которые он посылает на приводы движения:
- Значения ContinuousActions[0] и ContinuousActions[1] определяют вектор силы

Цель обучения - научить агент выбирать такие действия, чтобы максимизировать корреляцию между его состоянием (позиция цели и агента) и действиями (сила).
Чем выше эта корреляция, тем лучше агент сможет использовать информацию о своём состоянии для эффективного движения к цели.

## Задание 2
- Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.

batch_size:
- Определяет размер пакета (batch) обучающих примеров, использованных для обновления модели на каждой итерации.
- Увеличение значения batch_size может увеличить скорость обучения, так как модель будет обновляться на основе большего количества примеров за одну итерацию. Однако большие значения могут потребовать больше памяти и привести к более длительному времени обработки каждой итерации.

learning_rate:
- Определяет скорость обучения модели, то есть влияние каждого обновления на веса модели.
- Большое значение learning_rate может привести к быстрому обучению в начале процесса, но может также вызвать нестабильность и пропуск оптимальных решений. Маленькое значение может привести к более стабильному обучению, но замедлить процесс сходимости к оптимальным весам модели.
- 
buffer_size:
- Определяет размер буфера воспроизведения (replay buffer), который используется для хранения и повторного использования предыдущих опытов во время обучения.
- Увеличение значения buffer_size позволяет модели доступиться к большему количеству предыдущих опытов, что может помочь в улучшении обучения и стабильности. Большие значения требуют больше памяти, но также могут улучшить сходимость модели.


## Задание 3
- Привести примеры, для каких игровых задач и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения?

Пример 1. Игра среди соперников: ML-Agent может быть использован для обучения агента, который играет против других AI- или человеческих игроков. Например, агент может обучаться играть в шахматы, го или другие стратегические игры, где противниками являются другие агенты или игроки.

Пример 2. Управление несколькими роботами для сбора ресурсов в виртуальном мире.
При помощи ML-Аgent можно обучить координацию действий роботов для максимизации сбора ресурсов. Сложно написать подходящий алгоритм вручную.

ML-агент удобен, когда:
-Правила поведения трудно формализовать алгоритмически
-Нужно обучиться на основании опыта в сложной среде
-Действия зависят от множества факторов
-Требуется координация нескольких агентов
-Цель меняется или нечетко сформулирована

## Выводы
В ходе лабораторной работы я научилась работать с ML Agent и понимать его.

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
